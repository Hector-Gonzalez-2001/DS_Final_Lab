{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import our libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, make_scorer\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import keras\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load in our X (processed_fights.csv) and y (processed_data_labels.csv)\n",
        "X = pd.read_csv('data/processed/processed_fights.csv')\n",
        "y = pd.read_csv('data/processed/processed_data_labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = pd.read_csv('data/permutations/minus_all_11.csv')\n",
        "y = pd.read_csv('data/processed/processed_data_labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (4164, 24) (4164, 1)\n",
            "Testing set shape: (1042, 24) (1042, 1)\n",
            "Number of 0s in test = 532\n",
            "Number of 1s in test = 510\n"
          ]
        }
      ],
      "source": [
        "# Create our train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "print('Training set shape:', X_train.shape, y_train.shape)\n",
        "print('Testing set shape:', X_test.shape, y_test.shape)\n",
        "print('Number of 0s in test =', (y_test == 0).sum().sum())\n",
        "print('Number of 1s in test =', (y_test == 1).sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Standardize non-categorical features\n",
        "# features_to_scale = ['f1_reach', 'f1_sapm', 'f1_slpm',\n",
        "#        'f1_stk_acc', 'f1_stk_def', 'f1_sub_avg', 'f1_td_acc', 'f1_td_avg',\n",
        "#        'f1_td_def', 'f1_weight', 'f2_reach', 'f2_sapm', 'f2_slpm',\n",
        "#        'f2_stk_acc', 'f2_stk_def', 'f2_sub_avg', 'f2_td_acc', 'f2_td_avg',\n",
        "#        'f2_td_def', 'f2_weight', 'f1_wins', 'f1_losses', 'f1_draws', 'f2_wins',\n",
        "#        'f2_losses', 'f2_draws', 'f1_age', 'f2_age', 'f1_height', 'f2_height']\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
        "# X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
        "\n",
        "# # Save our scaler for later reference\n",
        "# with open('scaler.pkl', 'wb') as f:\n",
        "#     pickle.dump(scaler, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a logistic regression model\n",
        "lr = LogisticRegressionCV(penalty='l1', solver='liblinear', random_state=42)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Determine our logistic regression model accuracy and AUC\n",
        "y_pred = lr.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('Accuracy:', accuracy)\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (AUC = %0.2f)' % roc_auc)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression\n",
            "Accuracy: 0.6938579654510557\n",
            "AUC: 0.76\n",
            "\n",
            "K Neighbors\n",
            "Accuracy: 0.5921305182341651\n",
            "AUC: 0.64\n",
            "\n",
            "Decision Tree Classifier\n",
            "Accuracy: 0.6017274472168906\n",
            "AUC: 0.60\n",
            "\n",
            "RandomForest Classifier\n",
            "Accuracy: 0.6880998080614203\n",
            "AUC: 0.76\n",
            "\n",
            "GradientBoostingClassifier\n",
            "Accuracy: 0.6967370441458733\n",
            "AUC: 0.78\n",
            "\n",
            "AdaBoostClassifier\n",
            "Accuracy: 0.6813819577735125\n",
            "AUC: 0.76\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train on multiple models\n",
        "classifiers = [\n",
        "    LogisticRegressionCV(random_state=42),\n",
        "    KNeighborsClassifier(),\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    GradientBoostingClassifier(random_state=42),\n",
        "    AdaBoostClassifier(random_state=42),\n",
        "]\n",
        "\n",
        "classifiers_names = [\n",
        "    'Logistic Regression',\n",
        "    'K Neighbors',\n",
        "    'Decision Tree Classifier',\n",
        "    'RandomForest Classifier',\n",
        "    'GradientBoostingClassifier',\n",
        "    'AdaBoostClassifier'\n",
        "]\n",
        "\n",
        "for index, clf in enumerate(classifiers):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(classifiers_names[index])\n",
        "    print('Accuracy:', accuracy)\n",
        "    print('AUC: %0.2f' % roc_auc)\n",
        "    print()\n",
        "\n",
        "    with open('models/{}.pkl'.format(classifiers_names[index]), 'wb') as f:\n",
        "        pickle.dump(clf, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 4164 samples, validate on 1042 samples\n",
            "Epoch 1/100\n",
            "4164/4164 [==============================] - 1s 283us/sample - loss: 1.0583 - acc: 0.5646 - val_loss: 0.6875 - val_acc: 0.6190\n",
            "Epoch 2/100\n",
            "4164/4164 [==============================] - 0s 111us/sample - loss: 0.7009 - acc: 0.6311 - val_loss: 0.6733 - val_acc: 0.6180\n",
            "Epoch 3/100\n",
            "4164/4164 [==============================] - 0s 103us/sample - loss: 0.7226 - acc: 0.6256 - val_loss: 0.6623 - val_acc: 0.6449\n",
            "Epoch 4/100\n",
            "4164/4164 [==============================] - 0s 108us/sample - loss: 0.6417 - acc: 0.6602 - val_loss: 0.6212 - val_acc: 0.6622\n",
            "Epoch 5/100\n",
            "4164/4164 [==============================] - 0s 109us/sample - loss: 0.7457 - acc: 0.6314 - val_loss: 0.8417 - val_acc: 0.5979\n",
            "Epoch 6/100\n",
            "4164/4164 [==============================] - 0s 102us/sample - loss: 0.6703 - acc: 0.6448 - val_loss: 0.7369 - val_acc: 0.6036\n",
            "Epoch 7/100\n",
            "4164/4164 [==============================] - 0s 107us/sample - loss: 0.6736 - acc: 0.6386 - val_loss: 0.5930 - val_acc: 0.6785\n",
            "Epoch 8/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.6117 - acc: 0.6715 - val_loss: 0.6523 - val_acc: 0.6631\n",
            "Epoch 9/100\n",
            "4164/4164 [==============================] - 0s 109us/sample - loss: 0.6102 - acc: 0.6772 - val_loss: 0.6166 - val_acc: 0.6660\n",
            "Epoch 10/100\n",
            "4164/4164 [==============================] - 0s 107us/sample - loss: 0.6417 - acc: 0.6527 - val_loss: 0.6687 - val_acc: 0.6459\n",
            "Epoch 11/100\n",
            "4164/4164 [==============================] - 0s 108us/sample - loss: 0.6374 - acc: 0.6623 - val_loss: 0.6605 - val_acc: 0.6382\n",
            "Epoch 12/100\n",
            "4164/4164 [==============================] - 0s 107us/sample - loss: 0.6104 - acc: 0.6796 - val_loss: 0.6156 - val_acc: 0.6766\n",
            "Epoch 13/100\n",
            "4164/4164 [==============================] - 0s 104us/sample - loss: 0.6065 - acc: 0.6818 - val_loss: 0.6274 - val_acc: 0.6660\n",
            "Epoch 14/100\n",
            "4164/4164 [==============================] - 0s 107us/sample - loss: 0.5895 - acc: 0.6969 - val_loss: 0.6040 - val_acc: 0.6795\n",
            "Epoch 15/100\n",
            "4164/4164 [==============================] - 0s 108us/sample - loss: 0.5841 - acc: 0.6864 - val_loss: 0.6208 - val_acc: 0.6756\n",
            "Epoch 16/100\n",
            "4164/4164 [==============================] - 0s 107us/sample - loss: 0.5802 - acc: 0.6914 - val_loss: 0.6228 - val_acc: 0.6689\n",
            "Epoch 17/100\n",
            "4164/4164 [==============================] - 0s 109us/sample - loss: 0.5850 - acc: 0.6943 - val_loss: 0.7575 - val_acc: 0.6180\n",
            "Epoch 18/100\n",
            "4164/4164 [==============================] - 0s 107us/sample - loss: 0.6180 - acc: 0.6796 - val_loss: 0.7915 - val_acc: 0.5998\n",
            "Epoch 19/100\n",
            "4164/4164 [==============================] - 0s 107us/sample - loss: 0.5988 - acc: 0.6806 - val_loss: 0.6181 - val_acc: 0.6708\n",
            "Epoch 20/100\n",
            "4164/4164 [==============================] - 1s 127us/sample - loss: 0.5797 - acc: 0.6902 - val_loss: 0.6370 - val_acc: 0.6612\n",
            "Epoch 21/100\n",
            "4164/4164 [==============================] - 0s 102us/sample - loss: 0.5809 - acc: 0.6957 - val_loss: 0.6210 - val_acc: 0.6670\n",
            "Epoch 22/100\n",
            "4164/4164 [==============================] - 0s 103us/sample - loss: 0.5893 - acc: 0.6928 - val_loss: 0.6459 - val_acc: 0.6699\n",
            "Epoch 23/100\n",
            "4164/4164 [==============================] - 0s 99us/sample - loss: 0.5874 - acc: 0.6890 - val_loss: 0.6605 - val_acc: 0.6641\n",
            "Epoch 24/100\n",
            "4164/4164 [==============================] - 1s 144us/sample - loss: 0.5978 - acc: 0.6736 - val_loss: 0.6018 - val_acc: 0.6958\n",
            "Epoch 25/100\n",
            "4164/4164 [==============================] - 0s 104us/sample - loss: 0.5974 - acc: 0.6844 - val_loss: 0.6116 - val_acc: 0.6843\n",
            "Epoch 26/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5765 - acc: 0.6916 - val_loss: 0.6110 - val_acc: 0.6795\n",
            "Epoch 27/100\n",
            "4164/4164 [==============================] - 0s 103us/sample - loss: 0.5695 - acc: 0.7012 - val_loss: 0.6215 - val_acc: 0.6852\n",
            "Epoch 28/100\n",
            "4164/4164 [==============================] - 0s 99us/sample - loss: 0.5764 - acc: 0.7008 - val_loss: 0.6543 - val_acc: 0.6574\n",
            "Epoch 29/100\n",
            "4164/4164 [==============================] - 0s 99us/sample - loss: 0.5600 - acc: 0.7104 - val_loss: 0.6124 - val_acc: 0.6708\n",
            "Epoch 30/100\n",
            "4164/4164 [==============================] - 0s 102us/sample - loss: 0.5622 - acc: 0.7049 - val_loss: 0.7259 - val_acc: 0.6190\n",
            "Epoch 31/100\n",
            "4164/4164 [==============================] - 0s 97us/sample - loss: 0.5626 - acc: 0.7044 - val_loss: 0.6245 - val_acc: 0.6718\n",
            "Epoch 32/100\n",
            "4164/4164 [==============================] - 0s 101us/sample - loss: 0.5639 - acc: 0.6955 - val_loss: 0.6097 - val_acc: 0.6891\n",
            "Epoch 33/100\n",
            "4164/4164 [==============================] - 0s 103us/sample - loss: 0.5764 - acc: 0.6962 - val_loss: 0.6232 - val_acc: 0.6766\n",
            "Epoch 34/100\n",
            "4164/4164 [==============================] - 0s 100us/sample - loss: 0.5561 - acc: 0.7051 - val_loss: 0.7064 - val_acc: 0.6238\n",
            "Epoch 35/100\n",
            "4164/4164 [==============================] - 0s 97us/sample - loss: 0.5572 - acc: 0.7046 - val_loss: 0.5995 - val_acc: 0.6881\n",
            "Epoch 36/100\n",
            "4164/4164 [==============================] - 0s 105us/sample - loss: 0.5433 - acc: 0.7190 - val_loss: 0.6413 - val_acc: 0.6612\n",
            "Epoch 37/100\n",
            "4164/4164 [==============================] - 0s 96us/sample - loss: 0.5507 - acc: 0.7137 - val_loss: 0.6387 - val_acc: 0.6795\n",
            "Epoch 38/100\n",
            "4164/4164 [==============================] - 0s 104us/sample - loss: 0.5478 - acc: 0.7181 - val_loss: 0.6117 - val_acc: 0.6862\n",
            "Epoch 39/100\n",
            "4164/4164 [==============================] - 0s 99us/sample - loss: 0.5518 - acc: 0.7089 - val_loss: 0.5997 - val_acc: 0.6958\n",
            "Epoch 40/100\n",
            "4164/4164 [==============================] - 0s 105us/sample - loss: 0.5884 - acc: 0.6926 - val_loss: 0.5932 - val_acc: 0.6891\n",
            "Epoch 41/100\n",
            "4164/4164 [==============================] - 0s 105us/sample - loss: 0.5542 - acc: 0.7049 - val_loss: 0.6148 - val_acc: 0.6804\n",
            "Epoch 42/100\n",
            "4164/4164 [==============================] - 0s 97us/sample - loss: 0.5601 - acc: 0.7094 - val_loss: 0.6170 - val_acc: 0.6583\n",
            "Epoch 43/100\n",
            "4164/4164 [==============================] - 0s 108us/sample - loss: 0.5473 - acc: 0.7169 - val_loss: 0.6074 - val_acc: 0.7015\n",
            "Epoch 44/100\n",
            "4164/4164 [==============================] - 1s 124us/sample - loss: 0.5378 - acc: 0.7166 - val_loss: 0.6308 - val_acc: 0.6833\n",
            "Epoch 45/100\n",
            "4164/4164 [==============================] - 0s 100us/sample - loss: 0.5400 - acc: 0.7267 - val_loss: 0.6334 - val_acc: 0.6881\n",
            "Epoch 46/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5463 - acc: 0.7185 - val_loss: 0.6100 - val_acc: 0.6919\n",
            "Epoch 47/100\n",
            "4164/4164 [==============================] - 0s 102us/sample - loss: 0.5560 - acc: 0.7039 - val_loss: 0.6526 - val_acc: 0.6315\n",
            "Epoch 48/100\n",
            "4164/4164 [==============================] - 0s 102us/sample - loss: 0.5520 - acc: 0.7053 - val_loss: 0.6520 - val_acc: 0.6785\n",
            "Epoch 49/100\n",
            "4164/4164 [==============================] - 0s 101us/sample - loss: 0.5381 - acc: 0.7209 - val_loss: 0.6426 - val_acc: 0.6727\n",
            "Epoch 50/100\n",
            "4164/4164 [==============================] - 0s 90us/sample - loss: 0.5472 - acc: 0.7185 - val_loss: 0.6264 - val_acc: 0.6910\n",
            "Epoch 51/100\n",
            "4164/4164 [==============================] - 0s 93us/sample - loss: 0.5371 - acc: 0.7269 - val_loss: 0.6846 - val_acc: 0.6574\n",
            "Epoch 52/100\n",
            "4164/4164 [==============================] - 0s 88us/sample - loss: 0.5470 - acc: 0.7125 - val_loss: 0.6460 - val_acc: 0.6737\n",
            "Epoch 53/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5484 - acc: 0.7152 - val_loss: 0.6654 - val_acc: 0.6536\n",
            "Epoch 54/100\n",
            "4164/4164 [==============================] - 0s 100us/sample - loss: 0.5325 - acc: 0.7221 - val_loss: 0.6335 - val_acc: 0.6881\n",
            "Epoch 55/100\n",
            "4164/4164 [==============================] - 0s 100us/sample - loss: 0.5336 - acc: 0.7320 - val_loss: 0.6744 - val_acc: 0.6660\n",
            "Epoch 56/100\n",
            "4164/4164 [==============================] - 0s 103us/sample - loss: 0.5426 - acc: 0.7145 - val_loss: 0.6200 - val_acc: 0.6833\n",
            "Epoch 57/100\n",
            "4164/4164 [==============================] - 0s 103us/sample - loss: 0.5304 - acc: 0.7279 - val_loss: 0.6083 - val_acc: 0.6958\n",
            "Epoch 58/100\n",
            "4164/4164 [==============================] - 0s 99us/sample - loss: 0.5354 - acc: 0.7152 - val_loss: 0.6423 - val_acc: 0.6766\n",
            "Epoch 59/100\n",
            "4164/4164 [==============================] - 0s 101us/sample - loss: 0.5210 - acc: 0.7313 - val_loss: 0.6308 - val_acc: 0.6670\n",
            "Epoch 60/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5438 - acc: 0.7176 - val_loss: 0.6123 - val_acc: 0.6843\n",
            "Epoch 61/100\n",
            "4164/4164 [==============================] - 0s 105us/sample - loss: 0.5398 - acc: 0.7250 - val_loss: 0.6254 - val_acc: 0.6679\n",
            "Epoch 62/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5288 - acc: 0.7317 - val_loss: 0.6205 - val_acc: 0.6804\n",
            "Epoch 63/100\n",
            "4164/4164 [==============================] - 0s 105us/sample - loss: 0.5219 - acc: 0.7317 - val_loss: 0.6543 - val_acc: 0.6699\n",
            "Epoch 64/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5168 - acc: 0.7329 - val_loss: 0.6642 - val_acc: 0.6651\n",
            "Epoch 65/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5274 - acc: 0.7301 - val_loss: 0.6357 - val_acc: 0.6804\n",
            "Epoch 66/100\n",
            "4164/4164 [==============================] - 0s 105us/sample - loss: 0.5287 - acc: 0.7243 - val_loss: 0.6414 - val_acc: 0.6862\n",
            "Epoch 67/100\n",
            "4164/4164 [==============================] - 1s 121us/sample - loss: 0.5291 - acc: 0.7197 - val_loss: 0.6123 - val_acc: 0.6871\n",
            "Epoch 68/100\n",
            "4164/4164 [==============================] - 0s 112us/sample - loss: 0.5194 - acc: 0.7315 - val_loss: 0.6362 - val_acc: 0.6699\n",
            "Epoch 69/100\n",
            "4164/4164 [==============================] - 0s 102us/sample - loss: 0.5210 - acc: 0.7301 - val_loss: 0.6390 - val_acc: 0.6948\n",
            "Epoch 70/100\n",
            "4164/4164 [==============================] - 0s 99us/sample - loss: 0.5180 - acc: 0.7346 - val_loss: 0.6323 - val_acc: 0.6727\n",
            "Epoch 71/100\n",
            "4164/4164 [==============================] - 0s 103us/sample - loss: 0.5183 - acc: 0.7339 - val_loss: 0.6173 - val_acc: 0.6881\n",
            "Epoch 72/100\n",
            "4164/4164 [==============================] - 0s 102us/sample - loss: 0.5142 - acc: 0.7390 - val_loss: 0.6613 - val_acc: 0.6737\n",
            "Epoch 73/100\n",
            "4164/4164 [==============================] - 0s 105us/sample - loss: 0.5231 - acc: 0.7310 - val_loss: 0.7104 - val_acc: 0.6382\n",
            "Epoch 74/100\n",
            "4164/4164 [==============================] - 0s 104us/sample - loss: 0.5258 - acc: 0.7224 - val_loss: 0.6274 - val_acc: 0.6929\n",
            "Epoch 75/100\n",
            "4164/4164 [==============================] - 0s 102us/sample - loss: 0.5126 - acc: 0.7366 - val_loss: 0.6278 - val_acc: 0.6843\n",
            "Epoch 76/100\n",
            "4164/4164 [==============================] - 0s 99us/sample - loss: 0.5165 - acc: 0.7322 - val_loss: 0.6735 - val_acc: 0.6526\n",
            "Epoch 77/100\n",
            "4164/4164 [==============================] - 0s 103us/sample - loss: 0.5198 - acc: 0.7269 - val_loss: 0.6716 - val_acc: 0.6651\n",
            "Epoch 78/100\n",
            "4164/4164 [==============================] - 0s 105us/sample - loss: 0.5221 - acc: 0.7329 - val_loss: 0.6160 - val_acc: 0.6804\n",
            "Epoch 79/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5216 - acc: 0.7363 - val_loss: 0.6307 - val_acc: 0.6823\n",
            "Epoch 80/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5199 - acc: 0.7317 - val_loss: 0.6204 - val_acc: 0.6939\n",
            "Epoch 81/100\n",
            "4164/4164 [==============================] - 0s 104us/sample - loss: 0.5082 - acc: 0.7442 - val_loss: 0.6412 - val_acc: 0.6804\n",
            "Epoch 82/100\n",
            "4164/4164 [==============================] - 0s 105us/sample - loss: 0.5165 - acc: 0.7327 - val_loss: 0.6306 - val_acc: 0.6881\n",
            "Epoch 83/100\n",
            "4164/4164 [==============================] - 0s 108us/sample - loss: 0.5094 - acc: 0.7404 - val_loss: 0.6158 - val_acc: 0.6891\n",
            "Epoch 84/100\n",
            "4164/4164 [==============================] - 0s 104us/sample - loss: 0.5103 - acc: 0.7370 - val_loss: 0.6326 - val_acc: 0.6891\n",
            "Epoch 85/100\n",
            "4164/4164 [==============================] - 0s 104us/sample - loss: 0.5089 - acc: 0.7409 - val_loss: 0.6918 - val_acc: 0.6641\n",
            "Epoch 86/100\n",
            "4164/4164 [==============================] - 0s 100us/sample - loss: 0.5154 - acc: 0.7317 - val_loss: 0.6419 - val_acc: 0.6814\n",
            "Epoch 87/100\n",
            "4164/4164 [==============================] - 0s 104us/sample - loss: 0.5048 - acc: 0.7440 - val_loss: 0.6285 - val_acc: 0.6987\n",
            "Epoch 88/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5084 - acc: 0.7394 - val_loss: 0.6169 - val_acc: 0.6977\n",
            "Epoch 89/100\n",
            "4164/4164 [==============================] - 1s 128us/sample - loss: 0.5127 - acc: 0.7411 - val_loss: 0.6586 - val_acc: 0.6891\n",
            "Epoch 90/100\n",
            "4164/4164 [==============================] - 0s 108us/sample - loss: 0.5095 - acc: 0.7387 - val_loss: 0.6500 - val_acc: 0.6603\n",
            "Epoch 91/100\n",
            "4164/4164 [==============================] - 0s 107us/sample - loss: 0.5092 - acc: 0.7404 - val_loss: 0.7083 - val_acc: 0.6670\n",
            "Epoch 92/100\n",
            "4164/4164 [==============================] - 0s 106us/sample - loss: 0.5102 - acc: 0.7411 - val_loss: 0.6690 - val_acc: 0.6497\n",
            "Epoch 93/100\n",
            "4164/4164 [==============================] - 0s 103us/sample - loss: 0.4996 - acc: 0.7385 - val_loss: 0.6377 - val_acc: 0.6967\n",
            "Epoch 94/100\n",
            "4164/4164 [==============================] - 0s 100us/sample - loss: 0.5061 - acc: 0.7418 - val_loss: 0.6381 - val_acc: 0.6766\n",
            "Epoch 95/100\n",
            "4164/4164 [==============================] - 0s 101us/sample - loss: 0.4995 - acc: 0.7500 - val_loss: 0.6859 - val_acc: 0.6545\n",
            "Epoch 96/100\n",
            "4164/4164 [==============================] - 0s 102us/sample - loss: 0.5081 - acc: 0.7385 - val_loss: 0.6463 - val_acc: 0.6814\n",
            "Epoch 97/100\n",
            "4164/4164 [==============================] - 0s 108us/sample - loss: 0.4992 - acc: 0.7435 - val_loss: 0.6800 - val_acc: 0.6967\n",
            "Epoch 98/100\n",
            "4164/4164 [==============================] - 0s 104us/sample - loss: 0.5348 - acc: 0.7229 - val_loss: 0.6370 - val_acc: 0.6766\n",
            "Epoch 99/100\n",
            "4164/4164 [==============================] - 0s 107us/sample - loss: 0.5037 - acc: 0.7454 - val_loss: 0.6551 - val_acc: 0.6516\n",
            "Epoch 100/100\n",
            "4164/4164 [==============================] - 0s 102us/sample - loss: 0.4930 - acc: 0.7421 - val_loss: 0.7033 - val_acc: 0.6488\n",
            "Test accuracy: 0.6487524\n"
          ]
        }
      ],
      "source": [
        "# Define the layers of the neural network\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Make predictions on new data\n",
        "# predictions = model.predict(subtracted_fights_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "\n",
        "# # Load the preprocessed data and labels\n",
        "# data = pd.read_csv('new_processed_fights_data.csv')\n",
        "# #originalData = pd.read_csv('processed_fights.csv')\n",
        "# labels = pd.read_csv('processed_data_labels.csv')\n",
        "\n",
        "# # Add new feature to data set\n",
        "# #data['fighter1_win_pct'] = data['wins_fighter1'] / (data['wins_fighter1'] + data['losses_fighter1'])\n",
        "# #data['fighter2_win_pct'] = data['wins_fighter2'] / (data['wins_fighter2'] + data['losses_fighter2'])\n",
        "\n",
        "# # Drop the original wins, losses, and draws columns from the data set\n",
        "# #data = data.drop(columns=['wins_fighter1', 'losses_fighter1', 'draws_fighter1',\n",
        "# #                         'wins_fighter2', 'losses_fighter2', 'draws_fighter2'])\n",
        "\n",
        "# # Split the data and labels into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
        "\n",
        "# # Define the layers of the neural network\n",
        "# model = keras.Sequential([\n",
        "#     keras.layers.Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
        "#     keras.layers.Dense(32, activation='relu'),\n",
        "#     keras.layers.Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# # Evaluate the model on the testing set\n",
        "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "# print('Test accuracy:', test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensemble model accuracy: 0.6919385796545106\n"
          ]
        }
      ],
      "source": [
        "# Load the previously trained models\n",
        "with open('models/Logistic Regression.pkl', 'rb') as f:\n",
        "    lr = pickle.load(f)\n",
        "\n",
        "with open('models/GradientBoostingClassifier.pkl', 'rb') as f:\n",
        "    gbc = pickle.load(f)\n",
        "    \n",
        "with open('models/AdaBoostClassifier.pkl', 'rb') as f:\n",
        "    abc = pickle.load(f)\n",
        "\n",
        "with open('models/RandomForest Classifier.pkl', 'rb') as f:\n",
        "   rfc = pickle.load(f)\n",
        "\n",
        "with open('models/Decision Tree Classifier.pkl', 'rb') as f:\n",
        "   dtc = pickle.load(f)\n",
        "\n",
        "# Create the ensemble model\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('lr', lr),\n",
        "    ('gbc', gbc),\n",
        "    ('abc',abc),\n",
        "    # ('rfc',rfc),\n",
        "    # ('dtc',dtc)\n",
        "    ], voting='hard')\n",
        "\n",
        "# Fit the ensemble model to the training data\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set using the ensemble model\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the ensemble model\n",
        "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
        "print('Ensemble model accuracy:', accuracy_ensemble)\n",
        "\n",
        "with open('models/voteClass.pkl', 'wb') as f:\n",
        "    pickle.dump(ensemble_model, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "StackingClassifier.__init__() got an unexpected keyword argument 'classifier'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmlxtend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassifier\u001b[39;00m \u001b[39mimport\u001b[39;00m StackingClassifier\n\u001b[0;32m      3\u001b[0m \u001b[39m# Initialize the StackingClassifier\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m stacked_model \u001b[39m=\u001b[39m StackingClassifier(\n\u001b[0;32m      5\u001b[0m     classifier\u001b[39m=\u001b[39;49m[lr, gbc, abc, rfc, dtc],\n\u001b[0;32m      6\u001b[0m     meta_classifier\u001b[39m=\u001b[39;49mLogisticRegression(),\n\u001b[0;32m      7\u001b[0m     use_probas\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      8\u001b[0m     average_probas\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[39m# Train the stacked model using 5-fold cross-validation\u001b[39;00m\n\u001b[0;32m     12\u001b[0m skf \u001b[39m=\u001b[39m StratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: StackingClassifier.__init__() got an unexpected keyword argument 'classifier'"
          ]
        }
      ],
      "source": [
        "from mlxtend.classifier import StackingClassifier\n",
        "\n",
        "# Initialize the StackingClassifier\n",
        "stacked_model = StackingClassifier(\n",
        "    classifier=[lr, gbc, abc, rfc, dtc],\n",
        "    meta_classifier=LogisticRegression(),\n",
        "    use_probas=True,\n",
        "    average_probas=False\n",
        ")\n",
        "\n",
        "# Train the stacked model using 5-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for train_index, test_index in skf.split(X_train, y_train):\n",
        "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "    stacked_model.fit(X_train_fold, y_train_fold)\n",
        "    val_pred_fold = stacked_model.predict(X_val_fold)\n",
        "    print('Fold accuracy:', accuracy_score(y_val_fold, val_pred_fold))\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_pred = stacked_model.predict(X_test)\n",
        "print('Test set accuracy:', accuracy_score(y_test, test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import pickle\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Load the models\n",
        "# with open('lr.pkl', 'rb') as f:\n",
        "#     lr = pickle.load(f)\n",
        "# with open('gbc_gs.pkl', 'rb') as f:\n",
        "#     gbc = pickle.load(f)\n",
        "\n",
        "\n",
        "# # Load the data\n",
        "# X_test = pd.read_csv('processed_fights.csv')\n",
        "# y_test = pd.read_csv('processed_data_labels.csv')\n",
        "\n",
        "# # Make predictions on the test data\n",
        "# lr_pred = lr.predict(X_test)\n",
        "# gbc_pred = gbc.predict(X_test)\n",
        "\n",
        "# # Compute the accuracy of each model\n",
        "# lr_acc = accuracy_score(y_test, lr_pred)\n",
        "# gbc_acc = accuracy_score(y_test, gbc_pred)\n",
        "\n",
        "# # Print the accuracy of each model\n",
        "# print(f\"Logistic Regression Accuracy: {lr_acc}\")\n",
        "# print(f\"Gradient Boosting Accuracy: {gbc_acc}\")\n",
        "\n",
        "# # Create a blended prediction by taking a weighted average of the individual model predictions\n",
        "# blended_pred = (0.3 * lr_pred) + (0.5 * gbc_pred)\n",
        "\n",
        "# # Compute the accuracy of the blended prediction\n",
        "# blended_acc = accuracy_score(y_test, blended_pred)\n",
        "\n",
        "# # Print the accuracy of the blended prediction\n",
        "# print(f\"Blended Accuracy: {blended_acc}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
